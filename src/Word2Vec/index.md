# Word2vec

## 1. 背景：从维度灾难到分布式表示

对于复杂的自然语言任务进行建模，概率模型应运而生成为首选的方法，但在最开始的时候，学习语言模型的联合概率函数存在致命的维数灾难问题。假如语言模型的词典大小为100000，要表示10个连续词的联合分布，模型参数可能就要有10^50个。相应的，模型要具备足够的置信度，需要的样本量指数级增加。为了解决这个问题，最早是1986年Hinton等[5]提出分布式表示（Distributed Representation），基本思想是将词表示成n维连续的实数向量。分布式表示具备强大的特征表示能力，n维向量，每维有k个值，便能表示 k^n个特征。

## 2. Word2vec：核心思想与神奇特性

词向量是NLP深度学习研究的基石，本质上遵循这样的假设：**语义相似的词趋向于出现在相似的上下文**。因此在学习过程中，这些向量会努力捕捉词的邻近特征，从而学习到词汇之间的相似性。有了词向量，便能够通过计算余弦距离等方式来度量词与词之间的相似度。

![1](https://img.picui.cn/free/2025/06/21/6856ad3203f7f.png)
*图注: v(King) - v(Man) + v(Woman) ≈ v(Queen)*

Mikolov等[4]对词向量的广泛普及功不可没，他们用CBOW和Skip-gram模型训练出来的词向量具有神奇的语意组合特效，词向量的加减计算结果刚好是对应词的语意组合，譬如`v(King) - v(Man) + v(Woman) = v(Queen)`。这种意外的特效使得Word2vec快速流行起来。至于为什么会有这种行为呢？有意思的是Gittens等[10]做了研究，尝试给出了理论假设：词映射到低维分布式空间必须是均匀分布的，词向量才能有这个语意组合效果。

## 3. 实现模型：CBOW与Skip-gram

CBOW和Skip-gram是Word2vec的两种不同训练方式。CBOW（Continuous Bag-of-Words）指抠掉一个词，通过上下文预测该词；Skip-gram则与CBOW相反，通过一个词预测其上下文。

![1](https://img.picui.cn/free/2025/06/21/6856ad31e5bf0.png)
*图注: CBOW与Skip-gram模型结构对比*

以CBOW为例。CBOW模型是一个简单的只包含一个隐含层的全连接神经网络，输入层采用one-hot编码方式，词典大小为`V`；隐含层大小是`N`；输出层通过softmax函数得到词典里每个词的概率分布。层与层之间分别有两个权重矩阵`W ∈ R^(V×N)`和`W′ ∈ R^(N×V)`。词典里的每个单词都会学习到两个向量`vc`和`vw`，分别代表上下文向量和目标词向量。因此，给予上下文`c`，出现目标词`w`的概率是：`p(w|c)`，该模型需要学习的网络参数为`θ`。

![1](https://img.picui.cn/free/2025/06/21/6856ad321ffc1.png)
*图注: CBOW模型网络结构*

## 4. 局限与演进：从亚词（Subword）到上下文

词向量是用概率模型训练出来的产物，对训练语料库出现频次很低甚至不曾出现的词，词向量很难准确地表示。为了解决这个问题，研究者提出了**基于字级别的表征方式**，甚至更细粒度的**基于Byte级别的表征方式**，因为单词内的形态和形状信息同样有用。2017年，Mikolov等[9]提出用字级别的信息丰富词向量信息。此外，在机器翻译相关任务里，基于Byte级别的特征表示方式`BPE`（Byte Pair Encoding）还被用来学习不同语言之间的共享信息，主要以拉丁语系为主。2019年，Lample等[11]提出基于BERT优化的跨语言模型`XLM`，用BPE编码方式来提升不同语言之间共享的词汇量。

虽然通过词向量，一个单词能够很容易找到语义相似的单词，但单一词向量不可避免**一词多义**问题。对于一词多义问题，最常用有效的方式便是基于预训练模型，结合上下文表示单词，代表模型有**ELMo、GPT和BERT**，都能有效地解决这个问题。

## 5. 概念拓展：句子及篇章的向量表示

词，可以通过词向量进行表征，自然会有人思考分布式表示这个概念是否可以延伸到句子、文章、主题等。传统的词袋模型（Bag-of-Words）忽略词序以及词语义，为了解决这个问题，2014年Mikolov等[8]将分布式向量的表示扩展到句子和文章，提出了`Paragraph Vector`（常被称为`Doc2vec`），其模型训练类似word2vec。关于句子的向量表示，后续也涌现出如2015年Kiros等提出的`skip-thought`，以及2018年Logeswaran等提出的改进版`quick-thought`等模型。
