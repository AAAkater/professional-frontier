# BERT

## 1. 核心思想：深度双向Transformer

2018年，Devlin等[32]提出基于**深度双向Transformer**的预训练模型BERT（Bidirectional Encoder Representations from Transformers）。与GPT不同的是，BERT采用的特征提取器是Transformer的编码器（Encoder）部分。同样，BERT也分为**预训练（Pre-training）**和**下游任务微调（Fine-tuning）**两个阶段.

![a](https://img.picui.cn/free/2025/06/22/6857749b03347.png)
*图注: BERT的预训练与微调流程*

## 2. 输入表示：三位一体的Embedding

BERT的输入是一个线性序列，支持单句文本和句对文本。句首用特殊符号`[CLS]`表示，句尾用`[SEP]`表示，如果是句对，两个句子之间也用`[SEP]`分隔。 其输入特征由 **Token向量、Segment向量和Position向量** 三个部分相加而成，分别代表了单词信息、句子归属信息（第一句或第二句）和单词位置信息。

![b](https://img.picui.cn/free/2025/06/22/6857749b084bc.png)
*图注: BERT的输入表示*

## 3. 预训练任务

为了让模型能够理解深度双向的语言上下文以及句子间的关系，BERT采用了两种创新的预训练任务：**MLM** 和 **NSP**。

### 3.1 任务一：掩码语言模型 (Masked Language Model, MLM)

MLM的核心思想是，在输入的单词序列中，随机地**遮盖（mask）15%的单词**，然后让模型去预测这些被遮盖的单词。相比于传统语言模型只能从左到右或从右到左的单向预测，MLM允许模型同时利用左右两侧的上下文信息来进行预测，实现了真正的双向性。 不过，这种做法也带来了两个潜在问题:

1. **预训练与微调不匹配**：预训练阶段存在`[MASK]`符号，而下游任务微调时没有，这会造成不一致。
2. **收敛速度**：由于只对15%的单词进行预测，模型收敛需要更多的训练步数。

作者认为，MLM带来的巨大效果提升足以弥补收敛速度的损失。而对于第一个问题，他们提出了一种缓和策略：在被选中的15%的单词中，并不总是用`[MASK]`替换，而是进行如下处理（以句子“my dog is hairy”中“hairy”被选中为例）：

![c](https://img.picui.cn/free/2025/06/22/6857749ad5cb2.png)
*图注: MLM的80/10/10掩盖策略*

- **80%的概率**：用`[MASK]`符号替换该词，例如: "my dog is [MASK]"。
- **10%的概率**：用一个随机的词替换该词，例如: "my dog is apple"。
- **10%的概率**：保持原词不变，例如: "my dog is hairy"。 这样做的好处是，模型不仅要学会预测被遮盖的词，还要学会判断当前词是否被篡改，从而迫使模型学习到更具泛化能力的语境表示。

### 3.2 任务二：下一句预测 (Next Sentence Prediction, NSP)

许多重要的下游任务（如问答QA、自然语言推理NLI）都需要模型理解两个句子之间的逻辑关系。传统的语言模型在训练时并未显式地学习这种关系。 为此，BERT引入了NSP任务。在训练时，模型接收一对句子A和B，并需要预测B是否是A的下一句。

- **50%的概率**：B确实是A的下一句，标签为`IsNext`。
- **50%的概率**：B是语料库中随机挑选的一个句子，与A无关，标签为`NotNext`。 通过这个任务，BERT学会了捕捉句子级别的关系。

![d](https://img.picui.cn/free/2025/06/22/6857749aefad3.png)
*图注: NSP任务示意图*
